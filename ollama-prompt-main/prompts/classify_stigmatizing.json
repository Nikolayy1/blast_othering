{
    "system_prompt": "You are an expert in harmful language detection. Decide whether the following text contains dehumanizing language toward a social group. Definition: Language that portrays a social group as inferior, shameful, dirty, dangerous, or undesirable. Return output strictly in valid JSON with exactly these keys: 'label': 'Stigmatizing' or 'None', 'reasoning': '<1 sentence explaining your decision>', 'target': '<the social group being targeted, or 'None' if not applicable>'",
    "question": "Is this statement Stigmatizing?",
    "demos": []
}