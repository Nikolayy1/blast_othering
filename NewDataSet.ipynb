{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a952e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "from convokit import Corpus, download\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Method definitions\n",
    "def load_dataset_dynamic(corpus, start_index, end_index):\n",
    "    return Corpus(\n",
    "        filename=download(corpus),\n",
    "        backend=\"mem\",\n",
    "        utterance_start_index=start_index,\n",
    "        utterance_end_index=end_index,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_dataset_to_memory(corpus_path):\n",
    "    return Corpus(filename=corpus_path, backend=\"mem\")\n",
    "\n",
    "\n",
    "def get_target_ids(corpus):\n",
    "    target_ids = []\n",
    "\n",
    "    othering_terms = [\n",
    "        # Dehumanizing terms (animals, pests, disease metaphors)\n",
    "        \"animal\",\n",
    "        \"beast\",\n",
    "        \"savage\",\n",
    "        \"barbaric\",\n",
    "        \"subhuman\",\n",
    "        \"primitive\",\n",
    "        \"parasite\",\n",
    "        \"vermin\",\n",
    "        \"rat\",\n",
    "        \"cockroach\",\n",
    "        \"monster\",\n",
    "        \"brute\",\n",
    "        \"ape\",\n",
    "        \"gorilla\",\n",
    "        \"monkey\",\n",
    "        \"dog\",\n",
    "        \"pig\",\n",
    "        \"swine\",\n",
    "        \"goat\",\n",
    "        \"bug\",\n",
    "        \"leech\",\n",
    "        \"tick\",\n",
    "        \"lice\",\n",
    "        \"maggot\",\n",
    "        \"worm\",\n",
    "        \"cancer\",\n",
    "        \"tumor\",\n",
    "        \"virus\",\n",
    "        \"plague\",\n",
    "        \"infection\",\n",
    "        \"disease\",\n",
    "        # Moral judgment / worthlessness\n",
    "        \"scum\",\n",
    "        \"trash\",\n",
    "        \"filth\",\n",
    "        \"worthless\",\n",
    "        \"inferior\",\n",
    "        \"degenerate\",\n",
    "        \"lowlife\",\n",
    "        \"unworthy\",\n",
    "        \"unclean\",\n",
    "        \"impure\",\n",
    "        \"corrupt\",\n",
    "        \"dirty\",\n",
    "        \"disgusting\",\n",
    "        \"vile\",\n",
    "        \"evil\",\n",
    "        \"wicked\",\n",
    "        \"sinful\",\n",
    "        \"cursed\",\n",
    "        \"burden\",\n",
    "        \"freeloader\",\n",
    "        \"sponger\",\n",
    "        # Threatening / dangerous\n",
    "        \"dangerous\",\n",
    "        \"violent\",\n",
    "        \"aggressive\",\n",
    "        \"hostile\",\n",
    "        \"criminal\",\n",
    "        \"thug\",\n",
    "        \"deviant\",\n",
    "        \"radical\",\n",
    "        \"extremist\",\n",
    "        \"fundamentalist\",\n",
    "        \"terrorist\",\n",
    "        \"predator\",\n",
    "        \"rapist\",\n",
    "        \"gangster\",\n",
    "        \"invader\",\n",
    "        \"occupier\",\n",
    "        \"conqueror\",\n",
    "        \"infiltrator\",\n",
    "        \"threat\",\n",
    "        # Exclusion / separation\n",
    "        \"outsider\",\n",
    "        \"intruder\",\n",
    "        \"alien\",\n",
    "        \"stranger\",\n",
    "        \"foreigner\",\n",
    "        \"enemy\",\n",
    "        \"unwelcome\",\n",
    "        \"illegal\",\n",
    "        \"unwanted\",\n",
    "        \"expat\",\n",
    "        \"outcast\",\n",
    "        \"undesirable\",\n",
    "        \"colonizer\",\n",
    "        \"settler\",\n",
    "        # Diminishing / infantilizing\n",
    "        \"ignorant\",\n",
    "        \"backward\",\n",
    "        \"naive\",\n",
    "        \"uncivilized\",\n",
    "        \"helpless\",\n",
    "        \"weak\",\n",
    "        \"stupid\",\n",
    "        \"lazy\",\n",
    "        \"immature\",\n",
    "        \"childlike\",\n",
    "        \"silly\",\n",
    "        \"emotional\",\n",
    "        \"hysterical\",\n",
    "        \"irrational\",\n",
    "        \"clueless\",\n",
    "        \"brainwashed\",\n",
    "        \"sheep\",\n",
    "        \"puppet\",\n",
    "        \"follower\",\n",
    "        # Religion & secular identities\n",
    "        \"atheists\",\n",
    "        \"non-believers\",\n",
    "        \"secular people\",\n",
    "        \"buddhists\",\n",
    "        \"buddhist people\",\n",
    "        \"hindus\",\n",
    "        \"hindu people\",\n",
    "        \"christians\",\n",
    "        \"christian people\",\n",
    "        \"catholics\",\n",
    "        \"protestants\",\n",
    "        \"mormons\",\n",
    "        \"evangelicals\",\n",
    "        \"pagans\",\n",
    "        \"satanists\",\n",
    "        \"muslims\",\n",
    "        \"islamic people\",\n",
    "        \"islamists\",\n",
    "        \"muzzies\",\n",
    "        \"ragheads\",\n",
    "        \"jews\",\n",
    "        \"jewish people\",\n",
    "        \"zionists\",\n",
    "        \"orthodox jews\",\n",
    "        # Migration / nationality\n",
    "        \"immigrants\",\n",
    "        \"migrants\",\n",
    "        \"foreigners\",\n",
    "        \"outsiders\",\n",
    "        \"refugees\",\n",
    "        \"asylum seekers\",\n",
    "        \"expats\",\n",
    "        \"nationals\",\n",
    "        \"illegal aliens\",\n",
    "        \"illegals\",\n",
    "        \"invaders\",\n",
    "        \"colonizers\",\n",
    "        \"settlers\",\n",
    "        # Gender & women\n",
    "        \"women\",\n",
    "        \"woman\",\n",
    "        \"girl\",\n",
    "        \"girls\",\n",
    "        \"females\",\n",
    "        \"ladies\",\n",
    "        \"wives\",\n",
    "        \"mothers\",\n",
    "        \"bitches\",\n",
    "        \"sluts\",\n",
    "        \"whores\",\n",
    "        \"feminists\",\n",
    "        \"feminazis\",\n",
    "        # LGBTQ+\n",
    "        \"lgbtq\",\n",
    "        \"gay\",\n",
    "        \"gays\",\n",
    "        \"lesbian\",\n",
    "        \"lesbians\",\n",
    "        \"bisexual\",\n",
    "        \"transgender\",\n",
    "        \"trans\",\n",
    "        \"tranny\",\n",
    "        \"trannies\",\n",
    "        \"queer\",\n",
    "        \"queers\",\n",
    "        \"dyke\",\n",
    "        \"dykes\",\n",
    "        \"faggot\",\n",
    "        \"faggots\",\n",
    "        \"non-binary\",\n",
    "        \"drag queens\",\n",
    "        \"drag kings\",\n",
    "    ]\n",
    "\n",
    "    # Pre-compile regex patterns\n",
    "    patterns = [\n",
    "        re.compile(r\"(?i)(?<!\\w)\" + re.escape(term) + r\"(?!\\w)\")\n",
    "        for term in othering_terms\n",
    "    ]\n",
    "\n",
    "    for utt in corpus.iter_utterances():\n",
    "        if not utt.text:  # skip empty text\n",
    "            continue\n",
    "\n",
    "        # Count words\n",
    "        word_count = len(utt.text.split())\n",
    "\n",
    "        if word_count < 100 and any(p.search(utt.text) for p in patterns):\n",
    "            target_ids.append(utt.id)\n",
    "\n",
    "    return target_ids\n",
    "\n",
    "\n",
    "def get_id_chain(corpus, target_id):\n",
    "    chain = []\n",
    "    utt = corpus.get_utterance(target_id)\n",
    "    while utt is not None:\n",
    "        chain.append(utt)\n",
    "        utt = corpus.get_utterance(utt.reply_to) if utt.reply_to else None\n",
    "\n",
    "    # reverse so it's from root â†’ target\n",
    "    chain = chain[::-1]\n",
    "    return chain\n",
    "\n",
    "\n",
    "def plot_wordcloud(chain, title):\n",
    "    all_words = \" \".join(u.text for u in chain).lower()\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400, background_color=\"white\", stopwords=STOPWORDS\n",
    "    ).generate(all_words)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def export_comments_to_json(corpus, target_ids, filepath):\n",
    "    \"\"\"\n",
    "    Export selected comments into JSON format:\n",
    "    one big JSON object with numeric keys.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for idx, target_id in enumerate(target_ids, start=1):\n",
    "        utt = corpus.get_utterance(target_id)\n",
    "        if utt is None:\n",
    "            continue\n",
    "\n",
    "        # build id chain\n",
    "        id_chain = []\n",
    "        for i in get_id_chain(corpus, target_id):\n",
    "            id_chain.append({\"id\": i.id})\n",
    "\n",
    "        # get up to 3 preceding comments\n",
    "        last_idx = len(id_chain) - 1\n",
    "        start = max(0, last_idx - 3)\n",
    "        end = last_idx\n",
    "        context_ids = id_chain[start:end]\n",
    "\n",
    "        # collect context text\n",
    "        context_text = []\n",
    "        for entry in context_ids:\n",
    "            ctx_utt = corpus.get_utterance(entry[\"id\"])\n",
    "            if ctx_utt and ctx_utt.text:\n",
    "                context_text.append(ctx_utt.text)\n",
    "\n",
    "        # extract top keywords from context\n",
    "        context = categorize_chunks(\" \".join(context_text), top_n=10)\n",
    "\n",
    "        # build record\n",
    "        record = {\n",
    "            \"id\": target_id,\n",
    "            \"text\": getattr(utt, \"text\", None),\n",
    "            \"timestamp\": getattr(utt, \"timestamp\", None),\n",
    "            \"conversation_id\": getattr(utt, \"conversation_id\", None),\n",
    "            \"comment_chain\": id_chain,\n",
    "            \"context\": context,\n",
    "        }\n",
    "        results[idx] = record  # numeric keys\n",
    "\n",
    "    # save to JSON\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Saved {len(results)} comments to {filepath} (JSON format)\")\n",
    "\n",
    "\n",
    "\n",
    "def export_comments_to_jsonl(corpus, target_ids, filepath):\n",
    "    \"\"\"\n",
    "    Export selected comments into JSONL format:\n",
    "    one JSON object per line.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for target_id in target_ids:\n",
    "            utt = corpus.get_utterance(target_id)\n",
    "            if utt is None:\n",
    "                continue\n",
    "            \n",
    "            id_chain = []\n",
    "            context = []\n",
    "    \n",
    "            for i in get_id_chain(corpus, target_id):\n",
    "                id_chain.append({\"id\": i.id})\n",
    "    \n",
    "            last_idx = len(id_chain) - 1\n",
    "            start = max(0, last_idx - 3)  # up to 3 preceding comments\n",
    "            end = last_idx\n",
    "            context_ids = id_chain[start:end]\n",
    "\n",
    "            context_text = []\n",
    "            for id in context_ids:\n",
    "                context_text.append(corpus.get_utterance(id[\"id\"]).text)\n",
    "\n",
    "            context = categorize_chunks(\" \".join(context_text), top_n=10)\n",
    "\n",
    "            record = {\n",
    "            \"id\": target_id,\n",
    "            \"text\": getattr(utt, \"text\", None),\n",
    "            \"timestamp\": getattr(utt, \"timestamp\", None),\n",
    "            \"conversation_id\": getattr(utt, \"conversation_id\", None),\n",
    "            \"comment_chain\": id_chain,\n",
    "            \"context\": context,\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"âœ… Saved {count} comments to {filepath} (JSONL format)\")\n",
    "    \n",
    "def categorize_chunks(text, top_n=10):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # collect candidate words/phrases\n",
    "    candidates = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase = chunk.text.lower().strip()\n",
    "        if not all(token.is_stop for token in chunk):  # skip only-stopword chunks\n",
    "            candidates.append(phrase)\n",
    "\n",
    "    # deduplicate while preserving order\n",
    "    seen, keywords = set(), []\n",
    "    for c in candidates:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            keywords.append(c)\n",
    "\n",
    "    return keywords[:top_n]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Collapse multiple newlines into one space\n",
    "    text = re.sub(r\"\\s*\\n\\s*\", \" \", text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_links(text: str, keep_anchor=True):\n",
    "    # Remove markdown-style links [text](url)\n",
    "    if keep_anchor:\n",
    "        return re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", r\"\\1\", text)\n",
    "    else:\n",
    "        return re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", \"[URL]\", text)\n",
    "    \n",
    "def clean_html_tags(text: str) -> str:\n",
    "   return html.unescape(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6bff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at C:\\Users\\hofin\\.convokit\\saved-corpora\\reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "corpus = load_dataset_dynamic(\"reddit-corpus-small\", 0, 10000)\n",
    "target_ids = get_target_ids(corpus)\n",
    "\n",
    "# for target_id in target_ids:\n",
    "#     chain = get_id_chain(corpus, target_id)\n",
    "#     for u in chain:\n",
    "#         print(f\"{u.id}\")\n",
    "#     print(\"-----\")\n",
    "\n",
    "# print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f50cad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 17532 comments to data_collection/reddit-corpus-small.json (JSON format)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#target_ids = get_target_ids(corpus)\n",
    "\n",
    "os.chdir(r\"C:\\Users\\hofin\\OneDrive - Fachhochschule St. PÃ¶lten\\__BachelorThesis\\blast_othering\")\n",
    "\n",
    "\n",
    "export_comments_to_json(corpus, target_ids, \"data_collection/reddit-corpus-small.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f220876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved first 100\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\hofin\\OneDrive - Fachhochschule St. PÃ¶lten\\__BachelorThesis\\blast_othering\")\n",
    "\n",
    "with open(\"data_collection/reddit-corpus-small.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "# keep only the first `limit` items\n",
    "sliced = {k: data[k] for k in list(data.keys())[:100]}\n",
    "\n",
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sliced, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Saved first {len(sliced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0213da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key in data:\n",
    "    data[key][\"text\"] = clean_text(data[key][\"text\"])\n",
    "    data[key][\"text\"] = clean_links(data[key][\"text\"])\n",
    "    data[key][\"text\"] = clean_html_tags(data[key][\"text\"])\n",
    "    \n",
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f80982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved JSON to reddit_corpus.xlsx\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# Convert dict-of-dicts into a list of records\n",
    "records = []\n",
    "for key, value in data.items():\n",
    "    record = {\"id\": key}\n",
    "    record.update(value)\n",
    "    records.append(record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel(\"reddit_corpus.xlsx\", index=False)\n",
    "print(\"âœ… Saved JSON to reddit_corpus.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c7ecfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 11\n",
      "Text: Happened to me too. My first bf was at 17 and she was so afraid that the guy will rape me or something. Then we broke up and at 19 during my clubbing days she thought I was a hooker. Lol When I turned 21 with a bf she asked when Iâ€™m going to give her grandkids. Moved in but didnâ€™t marry at 24 and she got a shock when she found out we were sleeping on the same bed. What do you want, woman?!\n",
      "Reason: Uses dehumanizing language by comparing the person to an object (hooker) and later denies her humanity with 'What do you want, woman?!'\n",
      "-----\n",
      "ID: 16\n",
      "Text: > holding hands That's absolutely disgusting and you should be ashamed of yourself for such lewd behaviour. I need some brain-bleach now. Mods plz ban the poster permanently and banish her to the degenerate wasteland of r/handholding (NSFL link).\n",
      "Reason: Compares a person to something less than human, implying they are disgusting or worthless.\n",
      "-----\n",
      "ID: 25\n",
      "Text: You are backtracking. Someone posted a bunch of reasons why Singapore as a whole may be opposed to LGBT outside of religion and your response was this: > \"The only significant difference is that Singapore (as compared to India) has a much higher % of Christians and Catholics. So just to clarify - people from those two groups are bigots that preventing social progress. Let's call a spade a spade.\" Go ahead and try to shift the goal posts into \"whether Christians and Catholics are opposed to LGBT\". I'm done here.\n",
      "Reason: Compares people (Christians and Catholics) to objects (spades)\n",
      "-----\n",
      "ID: 51\n",
      "Text: Your ideal society must be pretty weak and unstable if you believe that it can be toppled by guys getting it on with one another.\n",
      "Reason: Compares people to objects (a society) based on sexual behavior.\n",
      "-----\n",
      "ID: 55\n",
      "Text: Answer my question about infertile people. And yes, people certainly want to choose to be executed, what a great choice. Just like how you choose to be straight, 10 year old kids also choose to be gay.\n",
      "Reason: Compares people with infertility to objects (execution) and denies their humanity by implying that sexual orientation is a choice.\n",
      "-----\n",
      "ID: 56\n",
      "Text: Answer my question about infertile people. Tell me, when did you choose to be straight? And being gay is so much more about sucking dicks, it is about developing love for the same gender. Just because you are incapable of love doesn't mean others are.\n",
      "Reason: Compares infertile people and those who are not gay to objects or diseases by denying them their human qualities.\n",
      "-----\n",
      "ID: 68\n",
      "Text: I can prove in microeconomics under asymmetry of information and wage determination that women should be paid lesser if we assume their periods make them less productive. I just had a friend who took mc because of cramps.\n",
      "Reason: Compares women to objects (less productive) based on a biological characteristic (periods)\n",
      "-----\n",
      "ID: 69\n",
      "Text: #I CAN PROVE IN MICROECONOMICS UNDER ASYMMETRY OF INFORMATION AND WAGE DETERMINATION THAT WOMEN SHOULD BE PAID LESSER IF WE ASSUME THEIR PERIODS MAKE THEM LESS PRODUCTIVE. I JUST HAD A FRIEND WHO TOOK MC BECAUSE OF CRAMPS.\n",
      "Reason: Compares women to objects (less productive) based on a biological characteristic (periods)\n",
      "-----\n",
      "ID: 96\n",
      "Text: Women used to be deemed unable to consent. And if you don't need animals' consent to kill them to eat, why do you need their consent to fuck them? Also virtually everyone who claims that all consensual sex should be allowed is against incest.\n",
      "Reason: Compares women to animals and denies their human qualities by implying they are unable to give consent.\n",
      "-----\n",
      "ID: 97\n",
      "Text: Have you seen gay pride parades in the US? Despite normalizing healthy gay relationships they are still deliberately culturally deviant\n",
      "Reason: Describes a group as culturally deviant, denying their humanity.\n",
      "-----\n",
      "ID: 98\n",
      "Text: maybe the answer is to make more things illegal. Like BDSM and foreplay. Only missionary. Maybe doggie cos animals do it so it is considered natural.\n",
      "Reason: Compares people to animals, suggesting that they should mimic animal behavior.\n",
      "-----\n",
      "ID: 100\n",
      "Text: What im saying face to face they dont complain as much. maybe in private or online. Compared to the sg. The sg im referring to arent foreigners, they are just there for studies riding on daddy's money. Most of the Malaysians and Filipinos are there using hard earned money they themselves earned.\n",
      "Reason: Compares a group (the 'sg') to objects ('riding on daddy's money'), denying their humanity.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/results/64/final_results_sorted_64.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# go into the \"data\" field of your JSON\n",
    "for key, entry in data[\"data\"].items():\n",
    "    if entry[\"annotation\"][\"label\"] == \"Dehumanizing\":\n",
    "        print(f\"ID: {key}\")\n",
    "        print(f\"Text: {entry['text']}\")\n",
    "        print(f\"Reason: {entry['annotation']['reasoning']}\")\n",
    "        print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e8a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
