{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a952e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hofin\\anaconda3\\envs\\bachelor_thesis\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderModel requires ML dependencies. Run 'pip install convokit[llm]' to install them.\n",
      "UnslothUtteranceSimulatorModel requires ML dependencies. Run 'pip install convokit[llm]' to install them.\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "import re\n",
    "from convokit import Corpus, download\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Method definitions\n",
    "def load_dataset_dynamic(corpus, start_index, end_index):\n",
    "    return Corpus(\n",
    "        filename=download(corpus),\n",
    "        backend=\"mem\",\n",
    "        utterance_start_index=start_index,\n",
    "        utterance_end_index=end_index,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_dataset_to_memory(corpus_path):\n",
    "    return Corpus(filename=corpus_path, backend=\"mem\")\n",
    "\n",
    "\n",
    "def get_target_ids(corpus):\n",
    "    target_ids = []\n",
    "\n",
    "    othering_terms = [\n",
    "        # Dehumanizing terms (animals, pests, disease metaphors)\n",
    "        \"animal\",\n",
    "        \"beast\",\n",
    "        \"savage\",\n",
    "        \"barbaric\",\n",
    "        \"subhuman\",\n",
    "        \"primitive\",\n",
    "        \"parasite\",\n",
    "        \"vermin\",\n",
    "        \"rat\",\n",
    "        \"cockroach\",\n",
    "        \"monster\",\n",
    "        \"brute\",\n",
    "        \"ape\",\n",
    "        \"gorilla\",\n",
    "        \"monkey\",\n",
    "        \"dog\",\n",
    "        \"pig\",\n",
    "        \"swine\",\n",
    "        \"goat\",\n",
    "        \"bug\",\n",
    "        \"leech\",\n",
    "        \"tick\",\n",
    "        \"lice\",\n",
    "        \"maggot\",\n",
    "        \"worm\",\n",
    "        \"cancer\",\n",
    "        \"tumor\",\n",
    "        \"virus\",\n",
    "        \"plague\",\n",
    "        \"infection\",\n",
    "        \"disease\",\n",
    "        # Moral judgment / worthlessness\n",
    "        \"scum\",\n",
    "        \"trash\",\n",
    "        \"filth\",\n",
    "        \"worthless\",\n",
    "        \"inferior\",\n",
    "        \"degenerate\",\n",
    "        \"lowlife\",\n",
    "        \"unworthy\",\n",
    "        \"unclean\",\n",
    "        \"impure\",\n",
    "        \"corrupt\",\n",
    "        \"dirty\",\n",
    "        \"disgusting\",\n",
    "        \"vile\",\n",
    "        \"evil\",\n",
    "        \"wicked\",\n",
    "        \"sinful\",\n",
    "        \"cursed\",\n",
    "        \"burden\",\n",
    "        \"freeloader\",\n",
    "        \"sponger\",\n",
    "        # Threatening / dangerous\n",
    "        \"dangerous\",\n",
    "        \"violent\",\n",
    "        \"aggressive\",\n",
    "        \"hostile\",\n",
    "        \"criminal\",\n",
    "        \"thug\",\n",
    "        \"deviant\",\n",
    "        \"radical\",\n",
    "        \"extremist\",\n",
    "        \"fundamentalist\",\n",
    "        \"terrorist\",\n",
    "        \"predator\",\n",
    "        \"rapist\",\n",
    "        \"gangster\",\n",
    "        \"invader\",\n",
    "        \"occupier\",\n",
    "        \"conqueror\",\n",
    "        \"infiltrator\",\n",
    "        \"threat\",\n",
    "        # Exclusion / separation\n",
    "        \"outsider\",\n",
    "        \"intruder\",\n",
    "        \"alien\",\n",
    "        \"stranger\",\n",
    "        \"foreigner\",\n",
    "        \"enemy\",\n",
    "        \"unwelcome\",\n",
    "        \"illegal\",\n",
    "        \"unwanted\",\n",
    "        \"expat\",\n",
    "        \"outcast\",\n",
    "        \"undesirable\",\n",
    "        \"colonizer\",\n",
    "        \"settler\",\n",
    "        # Diminishing / infantilizing\n",
    "        \"ignorant\",\n",
    "        \"backward\",\n",
    "        \"naive\",\n",
    "        \"uncivilized\",\n",
    "        \"helpless\",\n",
    "        \"weak\",\n",
    "        \"stupid\",\n",
    "        \"lazy\",\n",
    "        \"immature\",\n",
    "        \"childlike\",\n",
    "        \"silly\",\n",
    "        \"emotional\",\n",
    "        \"hysterical\",\n",
    "        \"irrational\",\n",
    "        \"clueless\",\n",
    "        \"brainwashed\",\n",
    "        \"sheep\",\n",
    "        \"puppet\",\n",
    "        \"follower\",\n",
    "        # Religion & secular identities\n",
    "        \"atheists\",\n",
    "        \"non-believers\",\n",
    "        \"secular people\",\n",
    "        \"buddhists\",\n",
    "        \"buddhist people\",\n",
    "        \"hindus\",\n",
    "        \"hindu people\",\n",
    "        \"christians\",\n",
    "        \"christian people\",\n",
    "        \"catholics\",\n",
    "        \"protestants\",\n",
    "        \"mormons\",\n",
    "        \"evangelicals\",\n",
    "        \"pagans\",\n",
    "        \"satanists\",\n",
    "        \"muslims\",\n",
    "        \"islamic people\",\n",
    "        \"islamists\",\n",
    "        \"muzzies\",\n",
    "        \"ragheads\",\n",
    "        \"jews\",\n",
    "        \"jewish people\",\n",
    "        \"zionists\",\n",
    "        \"orthodox jews\",\n",
    "        # Migration / nationality\n",
    "        \"immigrants\",\n",
    "        \"migrants\",\n",
    "        \"foreigners\",\n",
    "        \"outsiders\",\n",
    "        \"refugees\",\n",
    "        \"asylum seekers\",\n",
    "        \"expats\",\n",
    "        \"nationals\",\n",
    "        \"illegal aliens\",\n",
    "        \"illegals\",\n",
    "        \"invaders\",\n",
    "        \"colonizers\",\n",
    "        \"settlers\",\n",
    "        # Gender & women\n",
    "        \"women\",\n",
    "        \"woman\",\n",
    "        \"girl\",\n",
    "        \"girls\",\n",
    "        \"females\",\n",
    "        \"ladies\",\n",
    "        \"wives\",\n",
    "        \"mothers\",\n",
    "        \"bitches\",\n",
    "        \"sluts\",\n",
    "        \"whores\",\n",
    "        \"feminists\",\n",
    "        \"feminazis\",\n",
    "        # LGBTQ+\n",
    "        \"lgbtq\",\n",
    "        \"gay\",\n",
    "        \"gays\",\n",
    "        \"lesbian\",\n",
    "        \"lesbians\",\n",
    "        \"bisexual\",\n",
    "        \"transgender\",\n",
    "        \"trans\",\n",
    "        \"tranny\",\n",
    "        \"trannies\",\n",
    "        \"queer\",\n",
    "        \"queers\",\n",
    "        \"dyke\",\n",
    "        \"dykes\",\n",
    "        \"faggot\",\n",
    "        \"faggots\",\n",
    "        \"non-binary\",\n",
    "        \"drag queens\",\n",
    "        \"drag kings\",\n",
    "    ]\n",
    "\n",
    "    # Pre-compile regex patterns\n",
    "    patterns = [\n",
    "        re.compile(r\"(?i)(?<!\\w)\" + re.escape(term) + r\"(?!\\w)\")\n",
    "        for term in othering_terms\n",
    "    ]\n",
    "\n",
    "    for utt in corpus.iter_utterances():\n",
    "        if not utt.text:  # skip empty text\n",
    "            continue\n",
    "\n",
    "        # Count words\n",
    "        word_count = len(utt.text.split())\n",
    "\n",
    "        if word_count < 100 and any(p.search(utt.text) for p in patterns):\n",
    "            target_ids.append(utt.id)\n",
    "\n",
    "    return target_ids\n",
    "\n",
    "\n",
    "def get_id_chain(corpus, target_id):\n",
    "    chain = []\n",
    "    utt = corpus.get_utterance(target_id)\n",
    "    while utt is not None:\n",
    "        chain.append(utt)\n",
    "        utt = corpus.get_utterance(utt.reply_to) if utt.reply_to else None\n",
    "\n",
    "    # reverse so it's from root → target\n",
    "    chain = chain[::-1]\n",
    "    return chain\n",
    "\n",
    "\n",
    "def plot_wordcloud(chain, title):\n",
    "    all_words = \" \".join(u.text for u in chain).lower()\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400, background_color=\"white\", stopwords=STOPWORDS\n",
    "    ).generate(all_words)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def export_comments_to_json(corpus, target_ids, filepath):\n",
    "    \"\"\"\n",
    "    Export selected comments into JSON format:\n",
    "    one big JSON object with numeric keys.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for idx, target_id in enumerate(target_ids, start=1):\n",
    "        utt = corpus.get_utterance(target_id)\n",
    "        if utt is None:\n",
    "            continue\n",
    "\n",
    "        # build id chain\n",
    "        id_chain = []\n",
    "        for i in get_id_chain(corpus, target_id):\n",
    "            id_chain.append({\"id\": i.id})\n",
    "\n",
    "        # get up to 3 preceding comments\n",
    "        last_idx = len(id_chain) - 1\n",
    "        start = max(0, last_idx - 3)\n",
    "        end = last_idx\n",
    "        context_ids = id_chain[start:end]\n",
    "\n",
    "        # collect context text\n",
    "        context_text = []\n",
    "        for entry in context_ids:\n",
    "            ctx_utt = corpus.get_utterance(entry[\"id\"])\n",
    "            if ctx_utt and ctx_utt.text:\n",
    "                context_text.append(ctx_utt.text)\n",
    "\n",
    "        # extract top keywords from context\n",
    "        context = categorize_chunks(\" \".join(context_text), top_n=10)\n",
    "\n",
    "        # build record\n",
    "        record = {\n",
    "            \"id\": target_id,\n",
    "            \"text\": getattr(utt, \"text\", None),\n",
    "            \"timestamp\": getattr(utt, \"timestamp\", None),\n",
    "            \"conversation_id\": getattr(utt, \"conversation_id\", None),\n",
    "            \"comment_chain\": id_chain,\n",
    "            \"context\": context,\n",
    "        }\n",
    "        results[idx] = record  # numeric keys\n",
    "\n",
    "    # save to JSON\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Saved {len(results)} comments to {filepath} (JSON format)\")\n",
    "\n",
    "\n",
    "\n",
    "def export_comments_to_jsonl(corpus, target_ids, filepath):\n",
    "    \"\"\"\n",
    "    Export selected comments into JSONL format:\n",
    "    one JSON object per line.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for target_id in target_ids:\n",
    "            utt = corpus.get_utterance(target_id)\n",
    "            if utt is None:\n",
    "                continue\n",
    "            \n",
    "            id_chain = []\n",
    "            context = []\n",
    "    \n",
    "            for i in get_id_chain(corpus, target_id):\n",
    "                id_chain.append({\"id\": i.id})\n",
    "    \n",
    "            last_idx = len(id_chain) - 1\n",
    "            start = max(0, last_idx - 3)  # up to 3 preceding comments\n",
    "            end = last_idx\n",
    "            context_ids = id_chain[start:end]\n",
    "\n",
    "            context_text = []\n",
    "            for id in context_ids:\n",
    "                context_text.append(corpus.get_utterance(id[\"id\"]).text)\n",
    "\n",
    "            context = categorize_chunks(\" \".join(context_text), top_n=10)\n",
    "\n",
    "            record = {\n",
    "            \"id\": target_id,\n",
    "            \"text\": getattr(utt, \"text\", None),\n",
    "            \"timestamp\": getattr(utt, \"timestamp\", None),\n",
    "            \"conversation_id\": getattr(utt, \"conversation_id\", None),\n",
    "            \"comment_chain\": id_chain,\n",
    "            \"context\": context,\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "    print(f\"✅ Saved {count} comments to {filepath} (JSONL format)\")\n",
    "    \n",
    "def categorize_chunks(text, top_n=10):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # collect candidate words/phrases\n",
    "    candidates = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        phrase = chunk.text.lower().strip()\n",
    "        if not all(token.is_stop for token in chunk):  # skip only-stopword chunks\n",
    "            candidates.append(phrase)\n",
    "\n",
    "    # deduplicate while preserving order\n",
    "    seen, keywords = set(), []\n",
    "    for c in candidates:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            keywords.append(c)\n",
    "\n",
    "    return keywords[:top_n]\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Collapse multiple newlines into one space\n",
    "    text = re.sub(r\"\\s*\\n\\s*\", \" \", text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_links(text: str, keep_anchor=True):\n",
    "    # Remove markdown-style links [text](url)\n",
    "    if keep_anchor:\n",
    "        return re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", r\"\\1\", text)\n",
    "    else:\n",
    "        return re.sub(r\"\\[([^\\]]+)\\]\\([^)]+\\)\", \"[URL]\", text)\n",
    "    \n",
    "def clean_html_tags(text: str) -> str:\n",
    "   return html.unescape(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6bff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at C:\\Users\\hofin\\.convokit\\saved-corpora\\reddit-corpus-small\n"
     ]
    }
   ],
   "source": [
    "corpus = load_dataset_dynamic(\"reddit-corpus-small\", 0, 10000)\n",
    "target_ids = get_target_ids(corpus)\n",
    "\n",
    "# for target_id in target_ids:\n",
    "#     chain = get_id_chain(corpus, target_id)\n",
    "#     for u in chain:\n",
    "#         print(f\"{u.id}\")\n",
    "#     print(\"-----\")\n",
    "\n",
    "# print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f50cad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 17532 comments to data_collection/reddit-corpus-small.json (JSON format)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#target_ids = get_target_ids(corpus)\n",
    "\n",
    "os.chdir(r\"C:\\Users\\hofin\\OneDrive - Fachhochschule St. Pölten\\__BachelorThesis\\blast_othering\")\n",
    "\n",
    "\n",
    "export_comments_to_json(corpus, target_ids, \"data_collection/reddit-corpus-small.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f220876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved first 100\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\hofin\\OneDrive - Fachhochschule St. Pölten\\__BachelorThesis\\blast_othering\")\n",
    "\n",
    "with open(\"data_collection/reddit-corpus-small.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "# keep only the first `limit` items\n",
    "sliced = {k: data[k] for k in list(data.keys())[:100]}\n",
    "\n",
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sliced, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Saved first {len(sliced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a0213da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for key in data:\n",
    "    data[key][\"text\"] = clean_text(data[key][\"text\"])\n",
    "    data[key][\"text\"] = clean_links(data[key][\"text\"])\n",
    "    data[key][\"text\"] = clean_html_tags(data[key][\"text\"])\n",
    "    \n",
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f80982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved JSON to reddit_corpus.xlsx\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_collection/reddit-corpus-small-100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# Convert dict-of-dicts into a list of records\n",
    "records = []\n",
    "for key, value in data.items():\n",
    "    record = {\"id\": key}\n",
    "    record.update(value)\n",
    "    records.append(record)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save to Excel\n",
    "df.to_excel(\"reddit_corpus.xlsx\", index=False)\n",
    "print(\"✅ Saved JSON to reddit_corpus.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7ecfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 6\n",
      "Text: > if you think I talked to any of my friends in the manner he posted. Good thing I don't actually think that, but the fact that you're even fantasising and joking about it says a lot. Nah, you come off as a loser who doesn't know how to speak to women without scaring them off. That's a sad fact man. Now bye loser\n",
      "Reason: The comment contains an us-vs-them contrast by calling someone a 'loser' and implying they are unable to interact with women properly. The target group is framed as inferior.\n",
      "-----\n",
      "ID: 22\n",
      "Text: > The % of Muslims is similar in both countries. The only significant difference is that Singapore has a much higher % of Christians and Catholics. So just to clarify - people from those two groups are bigots that preventing social progress. Let's call a spade a spade. Using your reasoning, the higher % of Christians and Catholics are also responsible for reducing the prevalence of sex crimes. If you use that line of thinking it cuts both ways.\n",
      "Reason: The comment positions Christians and Catholics as bigots preventing social progress, framing them as 'other' in an us-vs-them contrast. This implies inferiority or exclusion of these groups.\n",
      "-----\n",
      "ID: 25\n",
      "Text: You are backtracking. Someone posted a bunch of reasons why Singapore as a whole may be opposed to LGBT outside of religion and your response was this: > \"The only significant difference is that Singapore (as compared to India) has a much higher % of Christians and Catholics. So just to clarify - people from those two groups are bigots that preventing social progress. Let's call a spade a spade.\" Go ahead and try to shift the goal posts into \"whether Christians and Catholics are opposed to LGBT\". I'm done here.\n",
      "Reason: The comment implies Christians and Catholics are bigots preventing social progress, positioning them as inferior to others who support LGBT rights.\n",
      "-----\n",
      "ID: 33\n",
      "Text: Well. Congrats on being an angry incel. At least you get access to public housing once you turn 35. I do feel that it sucks that the government doesn't provide more support to singles (especially single parents) when it comes to housing, but we were discussing about what discrimination gay couples face, not the issue of singles\n",
      "Reason: The comment contains an us-vs-them contrast by referencing 'angry incel' in a derogatory manner and implying that this group does not deserve access to public housing. The group is also positioned as inferior due to the mention of limited government support for their housing needs.\n",
      "-----\n",
      "ID: 40\n",
      "Text: Why do you presume that just because someone sees women as weaker, it means that they are looking down and mocking them? Your analogy is terrible because it has no similarities to NS. Again, women have the choice to join the army. Men don't.\n",
      "Reason: The comment positions women as inferior by implying they are weaker and have different opportunities than men, creating an us-vs-them contrast.\n",
      "-----\n",
      "ID: 43\n",
      "Text: you say it like the \"majority\" conforms at all by allowing gay people to bang it literally does not affect us, as the majority, in any way by being removed, unless you're gay enough to care what gay people do in their bedrooms\n",
      "Reason: The comment frames the LGBT community as 'other' by implying that they are outside of and separate from the majority. The use of the phrase 'unless you're gay enough to care' creates an us-vs-them contrast, positioning the LGBT community as inferior or excluded.\n",
      "-----\n",
      "ID: 48\n",
      "Text: So I assume the same goes for infertile people? Them having sex must be illegal? Since you rank the concept of family so highly, should all married couples be forced to reproduce as well?\n",
      "Reason: The comment implies that infertile people and gay people are not part of the traditional concept of family, which is highly valued. It suggests that their existence or actions could lead to negative consequences for society, using phrases like 'must be illegal' and 'crumble, society'. This creates an us-vs-them contrast where certain groups are positioned as inferior or outside belonging.\n",
      "-----\n",
      "ID: 49\n",
      "Text: And how did gays cause Islamic overpopulation and slow internet speeds? If you're trolling, you're not a very good one. If you're not, you're not very smart. Either way, it doesn't reflect well on you.\n",
      "Reason: The comment contains a derogatory tone and us-vs-them contrast towards individuals who identify as gay, positioning them as inferior or outside belonging.\n",
      "-----\n",
      "ID: 51\n",
      "Text: Your ideal society must be pretty weak and unstable if you believe that it can be toppled by guys getting it on with one another.\n",
      "Reason: The comment implies that a society where same-sex relationships are accepted is inherently 'weak' and 'unstable', positioning this group as inferior or outside belonging.\n",
      "-----\n",
      "ID: 55\n",
      "Text: Answer my question about infertile people. And yes, people certainly want to choose to be executed, what a great choice. Just like how you choose to be straight, 10 year old kids also choose to be gay.\n",
      "Reason: The comment positions certain groups as inferior or excluded, specifically targeting infertile people and homosexuals with derogatory language and implications of choice in sexual orientation.\n",
      "-----\n",
      "ID: 56\n",
      "Text: Answer my question about infertile people. Tell me, when did you choose to be straight? And being gay is so much more about sucking dicks, it is about developing love for the same gender. Just because you are incapable of love doesn't mean others are.\n",
      "Reason: The comment contains an us-vs-them contrast by positioning the speaker's group as capable of love and the other group as 'incapable of love', which is inferiorizing.\n",
      "-----\n",
      "ID: 57\n",
      "Text: This is so ridiculous that it should be on a comedy show. How do you explain the wars in the Middle East and other Muslim countries, then? Homosexuality is illegal there. &#x200B;\n",
      "Reason: The comment positions Muslims as 'other' by referencing their laws and cultural practices as inferior or ridiculous, implying a sense of superiority over them.\n",
      "-----\n",
      "ID: 59\n",
      "Text: For a start... &#x200B; No teaching of children about homosexuality No housing benefits which are specifically for straight couples Dont start suing businesses who refuse to participate in your gay wedding\n",
      "Reason: The comment expresses a clear us-vs-them contrast by advocating for discrimination against the LGBT community in areas such as education and housing benefits, positioning them as inferior or excluded.\n",
      "-----\n",
      "ID: 66\n",
      "Text: NS is long overdue an overhaul. NS is absolutely necessary but the way it was implemented can be improved. Women should serve in less physically intensive sectors. Higher pay and lower duration of NS. More exemptions should be made for sportsman etc. Culture should change as well, NS men are lacking the respect they deserve.\n",
      "Reason: The comment positions NS men as deserving of respect and suggests that they are lacking it, while implying that women should serve in less physically intensive sectors, creating an us-vs-them contrast.\n",
      "-----\n",
      "ID: 68\n",
      "Text: I can prove in microeconomics under asymmetry of information and wage determination that women should be paid lesser if we assume their periods make them less productive. I just had a friend who took mc because of cramps.\n",
      "Reason: The comment assumes a general difference in productivity between men and women due to biological factors, positioning women as less capable or inferior. This creates an us-vs-them contrast where women are seen as 'other' and excluded from equal pay.\n",
      "-----\n",
      "ID: 69\n",
      "Text: #I CAN PROVE IN MICROECONOMICS UNDER ASYMMETRY OF INFORMATION AND WAGE DETERMINATION THAT WOMEN SHOULD BE PAID LESSER IF WE ASSUME THEIR PERIODS MAKE THEM LESS PRODUCTIVE. I JUST HAD A FRIEND WHO TOOK MC BECAUSE OF CRAMPS.\n",
      "Reason: The comment assumes that women are less productive due to their periods, which is used as a basis for justifying lower pay. This creates an us-vs-them contrast where women are positioned as inferior and excluded from equal pay, based on gender-specific characteristics.\n",
      "-----\n",
      "ID: 72\n",
      "Text: I agree with your points based on an a genderless society where effort equals payoff. The majority of females don't do equal lifting of heavy stuff and take more leave/off for pregnancy or caregiving reasons. But applying this rule will end up with us being a very uncaring society. So feminists better don't keep harping for equality, you might just get what you wish for.\n",
      "Reason: The comment frames females as 'other' by implying they are inferior and less capable due to pregnancy or caregiving reasons. It also creates an us-vs-them contrast between males and females.\n",
      "-----\n",
      "ID: 73\n",
      "Text: I'm a guy and I oppose women serving NS, and I feel THAT's the non political correct thought lol, amongst men probably. I don't want the other half of the local population to waste 2 years of their lives just to make a point. Unless the govt deems it really necessary due to the declining population, then fair enough.\n",
      "Reason: The comment frames women as 'other' by implying they are not suited for National Service (NS) and that their participation would be a waste of time, reinforcing a gender-based stereotype.\n",
      "-----\n",
      "ID: 93\n",
      "Text: Because there are a lot of Singaporeans who are entitled. Anything less than perfect is a henious crime that shows the government isn't capable at all. Standards are far from realistic and like Nas said. Most Singaporeans lack perspective. For example. Everyone knows we are a \"resourceless\" nation. But to fully understand and comprehend the costs and danger of that is something that barely anyone begins to think about. Its scary to see how quickly how hateful, aggressive and in some cases, racist, some Singaporeans can become.\n",
      "Reason: The comment positions 'some Singaporeans' as inferior due to their alleged lack of perspective, entitlement, and potential for hateful or racist behavior, which creates an us-vs-them contrast.\n",
      "-----\n",
      "ID: 97\n",
      "Text: Have you seen gay pride parades in the US? Despite normalizing healthy gay relationships they are still deliberately culturally deviant\n",
      "Reason: The comment frames the LGBTQ+ community as 'other' by implying that they are culturally deviant despite efforts to normalize healthy gay relationships.\n",
      "-----\n",
      "ID: 99\n",
      "Text: Actually homophilic excesses are turning me off gay marriage\n",
      "Reason: The comment frames the LGBT+ community as 'other' by expressing a negative opinion about gay marriage due to perceived issues within the LGBT+ community itself, such as higher rates of suicide or discrimination. This creates an us-vs-them contrast and positions the group as inferior or problematic.\n",
      "-----\n",
      "ID: 100\n",
      "Text: What im saying face to face they dont complain as much. maybe in private or online. Compared to the sg. The sg im referring to arent foreigners, they are just there for studies riding on daddy's money. Most of the Malaysians and Filipinos are there using hard earned money they themselves earned.\n",
      "Reason: The comment creates an us-vs-them contrast between Singaporeans and other nationalities, particularly Malaysians and Filipinos. It positions Singaporeans as superior by implying that they are not 'riding on daddy's money' like the others, who are portrayed as inferior or less deserving.\n",
      "-----\n",
      "Total Othering entries: 22\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\hofin\\OneDrive - Fachhochschule St. Pölten\\__BachelorThesis\\blast_othering\")\n",
    "\n",
    "with open(\"data/results/76/stage_6_results_76.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# go into the \"data\" field of your JSON\n",
    "for key, entry in data[\"data\"].items():\n",
    "    if entry[\"annotation\"][\"label\"] == \"Othering\":\n",
    "        print(f\"ID: {key}\")\n",
    "        print(f\"Text: {entry['text']}\")\n",
    "        print(f\"Reason: {entry['annotation']['reasoning']}\")\n",
    "        print(\"-----\")\n",
    "        \n",
    "print(f\"Total Othering entries: {len([entry for entry in data['data'].values() if entry['annotation']['label'] == 'Othering'])}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e8a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
