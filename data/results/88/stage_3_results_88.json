{
    "config": {
        "config": "default.yaml",
        "prompt_file_stage_1": "prompts/default.json",
        "prompt_file_stage_2": "prompts/classify_harm.json",
        "dataset": "stage_3_data_88.json",
        "out_filename": "stage_3_results_88.json",
        "model": "llama3.3:70b-instruct-q4_0",
        "seed": 42,
        "temperature": 0.15,
        "host": "10.225.144.50",
        "port": "9999",
        "workers": 4,
        "save_interval": 20,
        "max_retries": 3,
        "prompt_file_dehumanizing": "prompts/classify_dehumanizing.json",
        "prompt_file_stigmatizing": "prompts/classify_stigmatizing.json",
        "prompt_file_stereotyping": "prompts/classify_stereotyping.json",
        "prompt_file_simplifying": "prompts/classify_simplifying.json",
        "prompt_file_othering": "prompts/classify_othering.json"
    },
    "time_saved": "2025-10-16 22:39:21",
    "prompt_data": {
        "system_prompt": "You are an expert in harmful language detection. Decide whether the following text contains dehumanizing language toward a social group. Definition: Language that portrays a social group as inferior, shameful, dirty, dangerous, or undesirable. Return output strictly in valid JSON with exactly these keys: 'label': 'Stigmatizing' or 'None', 'reasoning': '<1 sentence explaining your decision>', 'target': '<the social group being targeted, or 'None' if not applicable>'",
        "question": "Is this statement Stigmatizing?",
        "demos": []
    },
    "data": {}
}