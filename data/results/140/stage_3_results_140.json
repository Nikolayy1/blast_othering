{
    "config": {
        "config": "default.yaml",
        "prompt_file_stage_1": "prompts/default.json",
        "prompt_file_stage_2": "prompts/classify_harm.json",
        "dataset": "stage_3_data_140.json",
        "out_filename": "stage_3_results_140.json",
        "model": "llama3.3:70b-instruct-q4_0",
        "seed": 42,
        "temperature": 0.15,
        "host": "127.0.0.1",
        "port": "9999",
        "workers": 4,
        "save_interval": 50,
        "max_retries": 3,
        "prompt_file_dehumanizing": "prompts/classify_dehumanizing.json",
        "prompt_file_stigmatizing": "prompts/classify_stigmatizing.json",
        "prompt_file_stereotyping": "prompts/classify_stereotyping.json",
        "prompt_file_simplifying": "prompts/classify_simplifying.json",
        "prompt_file_othering": "prompts/classify_othering.json",
        "prompt_file_othering_identify_target": "prompts/classify_othering_identify_target.json",
        "prompt_file_othering_is_social_group": "prompts/classify_othering_is_social_group.json",
        "prompt_file_othering_portrayal": "prompts/classify_othering_portrayal.json",
        "prompt_file_othering_identify_unified_group": "prompts/classify_othering_identify_unified_group.json"
    },
    "time_saved": "2025-11-02 22:23:02",
    "prompt_data": {
        "system_prompt": "You are an expert in harmful language detection. Decide whether the following text contains dehumanizing language toward a social group. Definition: Language that portrays a social group as inferior, shameful, dirty, dangerous, or undesirable. Return output strictly in valid JSON with exactly these keys: 'label': 'Stigmatizing' or 'None', 'reasoning': '<1 sentence explaining your decision>', 'target': '<the social group being targeted, or 'None' if not applicable>'",
        "question": "Is this statement Stigmatizing?",
        "demos": []
    },
    "data": {}
}